<!DOCTYPE html>
<html>
  <head>
    <title>Digital Human</title>
    <meta http-equiv="content-type" charset="utf-8">
    <meta keywords="Digital Human">
    <meta description="Digital Human Project">
  </head>
  <body>
  <h1>Practical Digital Human Project</h1>


  <h2>Quick Introduction</h2>
  <video src="./MeshLab_2021-04-08.mp4" poster="./MeshLab_2021-04-08.jpg" controls width="30%"></video><br>
  In this page, I will try to break down the digital human technology by using the similar workflow as game production is using. In order to achieve realistic digital human, it is necessary to combine various technologies, and I believe that just digging each of them will be enough for research topic. We would like to present an application paper if we get some results. We are not looking for innovative algorithms, but we focus on build a robust pipeline to achive the current state of the art digital human.<br>
  
  We introduce some useful links to understand what current game industry is doing.<br>
  1) <a href="https://techblog.sega.jp/entry/2018/12/26/100000">龍が如くにおけるキャラクター制作ワークフロー</a><br>
  2) <a href="https://cedil.cesa.or.jp/cedil_sessions/view/1545">BIOHAZARD 7 - PHOTOGRAMMETRY -</a><br>
  3) <a href="https://magazine.cygames.co.jp/archives/600">スキャンスタジオ【社内設備特集Vol.1】</a><br>
  4) <a href="https://magazine.cygames.co.jp/archives/14013">「LightCage」採用の最先端フェイススキャナー導入！開発と直結した圧倒的フォトリアルな素材づくりに迫る【社内設備特集Vol.4】</a><br>
  5) <a href="https://developer.nvidia.com/gtc/2020/video/s22115-vid">Generative Face Models from Light Stage Scans</a><br>
  6) <a href="https://cgworld.jp/feature/201908-gg2019-dmc5.html">フォトグラメトリーでつくる『デビル メイ クライ 5』のキャラクターモデル</a><br>
  7) <a href="https://cedil.cesa.or.jp/cedil_sessions/view/1321">プロ野球スピリッツ2015の3Dフォトスキャン活用事例</a><br>
  8) <a href="https://www.youtube.com/watch?v=_OuCrbwEJW4">Creating Believable Characters in Unreal Engine</a><br>

  <h2>Captruing Devices & 3D Reconstruction</h2>
  <img src="./S__37306371.jpg" width="20%"> <img src="./2021-05-09.jpg" width="20%"> <img src="./metashape.jpg" width="23%"><br>
  Recenly, many game companies are building own capturing studio using 30-50 DSLRs. They send captured images to <a href="https://www.capturingreality.com/">RealityCapture</a> or <a href="https://oakcorp.net/agisoft/">PhotoScan</a>. These softwares are finding matching point, estimating camera parameters, and finally building a large number of point clouds by solving ICP algorithm.<br>
  We are considering a technique using stereo matching algorighm and a few DSLRs with cross polarized light as an alternative to the above approach. This is mostly inspired by some disney rsearch works. For capturing device, we plan to use Canon EOS Kiss X7i with EF50mm F1.8 STM lense and 49mm polarized filter.
  Until we have good state of art stereo matching algorithm, we plan to use RealityCapture to get point clouds. Commercial middleware is based on accumulated know-how, so it can provide stable and high performance results, but we would like to develop our own algorithm.<br>
  1)<a href="https://studios.disneyresearch.com/2020/08/14/single-shot-high-quality-facial-geometry-and-skin-appearance-capture/">Single-Shot High-Quality Facial Geometry and Skin Appearance Capture</a><br>
  2)<a href="https://studios.disneyresearch.com/wp-content/uploads/2019/03/Practical-Dynamic-Facial-Appearance-Modeling-and-Acquisition-Paper1.pdf">Practical Dynamic Facial Appearance Modeling and Acquisition</a><br>

  <h2>Shrink & Wrapping</h2>
  <img src="./arap.jpg" width="30%"> <br>
  Once you have a large number of point clouds, we will apply Shrink & Wrap algorithm. Most productions nowadays have a base mesh that serves as a template for the character, organized by the designer in a neat topology, which can be fitted to the point cloud to control the polygon count budget. Production often uses <a href="https://knowledge.autodesk.com/ja/support/maya/learn-explore/caas/CloudHelp/cloudhelp/2018/JPN/Maya-Modeling/files/GUID-832864E7-A266-4EA8-92A1-7D570759D74E-htm.html">Maya</a> or <a href="https://www.russian3dscanner.com/">R3DS Wrap</a> to fit data and we assume they use "<b>as rigid as possible (ARAP)</b>" algorithm. <br>
  Until we have good state of ARAP algorithm, we plan to use Maya or R3DS to fit our model to point clouds. Commercial middleware is based on accumulated know-how, so it can provide stable and high performance results, but we would like to develop our own algorithm.<br>
  1)<a href="http://www.cs.tau.ac.il/~levin/arap.pdf">As-Rigid-As-Possible Shape Interpolation</a><br>

  <h2>Normal Map & Roughness Map</h2>
  In physics based shading, single normal map is ideal solution, but getting an accurate signel normal map for a face is difficult, and there are many ways to get normal maps in different levels.  Debevec says photometric stereo solution of albedo normal gave only insufficient results compared to specular normal that can derive the mesostrcuture properly. And this specular normal approache is accepted as the standard in the film industry. On the other hand, the game industry has not yet used such an expensive workflow and is still deriving approximated gometry with albedo map based on stereo matching, then derive albedo normal using <a href="https://xnormal.net/">xNormal</a> and compute roughness map using <a href="http://boundingboxsoftware.com/materialize/">Materialize</a>. The table below is a simple summary of this area.
  <table border="1">
    <tr>
      <th>Quality</th>
      <th>Approch</th>
      <th>Devices</th>
      <th>Shots</th>
      <th>Reference</th>
    </tr>
    <tr>
      <td>◎</td>
      <td>Specular Normal using Photometric Stereo<br>(<b>Used in Movie Industry</b>)</td>
      <td>Programmable Light + <br>Cross/Parallel polarization filter</td>
      <td>High Speed Burst Shot</td>
      <td><a href="https://vgl.ict.usc.edu/Research/Multiview/">Multiview Face Capture using Polarized Spherical Gradient Illumination (Siggraph 2011)</a></td>
    </tr>
    <tr>
      <td>○</td>
      <td>Specular Normal using Cross/Parallel polarization</td>
      <td>Cross/Parallel polarization filter</td>
      <td>Single Shot</td>
      <td><a href="https://studios.disneyresearch.com/2020/08/14/single-shot-high-quality-facial-geometry-and-skin-appearance-capture/">Single-Shot High-Quality Facial Geometry and Skin Appearance Capture (Siggraph 2020)</a></td>
    </tr>
    <tr>
      <td>△</td>
      <td>Albedo Normal using Photometric Stereo</td>
      <td>Programmable Light + <br>Cross polarization filter</td>
      <td>High Speed Burst Shot</td>
      <td><a href="http://cvl.ist.osaka-u.ac.jp/wp-content/uploads/2020/04/cao2020stereoscopic.pdf">Stereoscopic Flash and No-Flash Photography for Shape and Albedo Recovery (CVPR 2020)</a></td>
    </tr>
    <tr>
      <td>▲</td>
      <td>Albedo Normal using Patch Match Stereo<br>(<b>Used in Game Industry</b>)</td>
      <td>Cross polarization filter</td>
      <td>Single Shot</td>
      <td><a href="https://cgl.ethz.ch/publications/papers/paperBee10.php">High-Quality Single-Shot Capture of Facial Geometry (Siggraph 2010)</a></td>
    </tr>
  </table><br>
  1)<a href="https://vgl.ict.usc.edu/Research/FaceScanning/EGSR2007_SGI_high.pdf">Rapid Acquisition of Specular and Diffuse Normal Maps from Polarized Spherical Gradient Illumination</a><br>

  <h2>Facial Animation</h2>
  <video src="./SimpleBlendShape_2021-04-13.mp4" poster="./SimpleBlendShape_2021-04-13.jpg" controls width="30%"></video><br>
  Facial animation is completely different topics. facial animation is basically creating blendshapes that complement to create facial expressions. In academia, we often shoot with 20-30 typical facial expressions and mix them together to create animations.
  However, just note the actual production process is much more complicated than this, including the generation of huge number of blendshapes, situational rigs, and hand-applied animation. <br>
  1)<a href="https://www.slideshare.net/capcom_rd/ss-139179580">緻密なキャラクターの表情や破壊表現のためのコンピュートシェーダによるメッシュアニメーション</a><br>
  2)<a href="https://github.com/zhuhao-nju/facescape">FaceScape: a Large-Scale High Quality 3D Face Dataset and Detailed Riggable 3D Face Prediction</a><br>

  <h2>Real Time Rendering</h2>
  <img src="./ue4rendering.jpg" width="30%"><br>
  Game developers love real-time rendering with shaders, so there has been a lot of research on human rendering. Real-time renderers are reasonably hard to write, so it may be more efficient to simply follow the <a href="https://docs.unrealengine.com/ja/Resources/Showcases/DigitalHumans/index.html">Unreal Engine 4 digital human system</a> to keep the cost of the work down. Without having to write huge amounts of shaders, it is possible to move realistic faces by simply pulling in texture and animation data in blueprints. However, if you eventually want to make a real-time renderer for these areas using DirectX or Vulkan, please try it. I think it will be very advantageous for you to get a job!<br>
 1) <a href="http://www.iryoku.com/stare-into-the-future">NEXT GENERATION LIFE</a><br>
 2) <a href="https://www.gdcvault.com/play/1018270/Next-Generation-Character">Next Generation Character Rendering</a><br>


  <h2>Hair Modeling & Simulation</h2>
  <img src="./maya_xgen.jpg" width="30%"><br>
  Hair Modeling is another completely different topic. Typically, we model strands by hand, looking at pictures, using <a href="https://www.youtube.com/watch?v=rfxt0ubgLXc">Maya xGen</a> or <a href="https://ephere.com/plugins/autodesk/maya/ornatrix/">Ornatrix</a>. There is a lot of research on automating the creation of strands from photos, but I get the impression that production never used these algorithms and that everything is created by artist hand. I guess there is a huge quality gap between academic and prodcution. If you want to work into this field, it is important to have a sincere attitude towards production.<br>
  As for hair simulation, EA frostbite are spending time for this area. These area will requre a lot of fundamental knowldges of physical animation area and some of you want to do that.<br>
 1) <a href="https://advances.realtimerendering.com/s2019/hair_presentation_final.pdf">Strand-based Hair Rendering in Frostbite</a><br>
 2) <a href="https://www.ea.com/frostbite/news/the-future-of-hair-rendering-technology-in-frostbite">How Frostbite is Advancing the Future of Hair Rendering Technology</a><br>

  <h2>Teeth & Eye capturing and simulation</h2>
  We may touch these topics some day. However, I don't think that the eyes and teeth are something that will dramatically change the impression, and I think that a certain level of quality can be ensured by using template models. Disney research are doing a lot of these capture researches including eye or teeth capturing.<br>
 1) <a href="https://studios.disneyresearch.com/category/capture/">Disney Research Studio</a><br>

  <h2>Others</h2>
  Realistic human is more trending. <br>
  1) <a href="https://www.unrealengine.com/ja/digital-humans">MetaHumans</a><br>
  2) <a href="https://www.reallusion.com/jp/character-creator/">Character Creator</a><br>


  <h4>email: ShinichiKinuwaki [at] gmail.com</h4>
  </body>
</html>